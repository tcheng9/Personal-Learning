{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing Logisitic Regression from scratch\n",
    "#https://towardsdatascience.com/logistic-regression-from-scratch-69db4f587e17\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sigmoid function g(z) can be used to represent an actual prediction y_hat. After the moedl is made, we can calculate the loss function (called \"cross-entrophy\").\n",
    "\n",
    "Note the loss function is different depending on whether y is 0 or y is 1.\n",
    "\n",
    "If y = 1, Loss function = -log(y_hat)\n",
    "If y = 0, Loss function = -log(1-y_hat)\n",
    "\n",
    "From these functions, it can shown that these two equations can be combined into a single line equation\n",
    "\n",
    "L(y,y_hat) - single line equation: (-1/m) *(sum of)[yi * log(y_hat) + (1-y)*log(1-y_hat)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example of updating one weight\n",
    "\n",
    "epochs = 50\n",
    "lr = 0.1\n",
    "\n",
    "for _ in range(epochs):\n",
    "    w1 -= lr * x1 * (y_hat - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import log, dot, e\n",
    "from numpy.random import rand\n",
    "\n",
    "class LogisticRegression:\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return 1/(1+e**(-z))\n",
    "    \n",
    "    def cost_function(self, X,y, weights):\n",
    "        z = dot(X, weights)\n",
    "        predict_1 = y*log(self.sigmoid(z)) \n",
    "        predict_0 = (1-y) * log(1-self.sigmoid(z)) \n",
    "        return -sum(predict_1+predict_0)/len(X)\n",
    "\n",
    "    def fit(self, X,y, epochs = 25, lr = 0.05):\n",
    "        loss = []\n",
    "        weights = rand(X.shape[1])\n",
    "        N = len(X)\n",
    "        \n",
    "        for _ in range(epochs):\n",
    "            #Gradient descent\n",
    "            y_hat = self.sigmoid(dot(X, weights))\n",
    "            weights -= lr * dot(X.T, y_hat - y)/N\n",
    "            \n",
    "            #Saving progress\n",
    "            loss.append(self.cost_function(X,y,weights))\n",
    "        \n",
    "        self.weights= weights\n",
    "        self.loss = loss\n",
    "    \n",
    "    def predict(self, X):\n",
    "        z = dot(X,self.weights)\n",
    "        return[1 if i > 0.5 else 0 for i in self.sigmoid(z)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   def predict(self, X):        \n",
    "        # Predicting with sigmoid function\n",
    "        z = dot(X, self.weights)\n",
    "        # Returning binary result\n",
    "        return [1 if i > 0.5 else 0 for i in self.sigmoid(z)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot predictions/loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Things to do\n",
    "\n",
    "#Relearn cost function, why was there \"predict_1 and predict 0\"? Isn't predict_0 the right equation?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
